{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52eb12e9",
   "metadata": {},
   "source": [
    "# Uczenie ze wzmocnieniem\n",
    "## Miłosz Mizak\n",
    "\n",
    "### 1. Wstęp\n",
    "\n",
    "Przedmiotem szóstego zadania była implementacja algorytmu Q-learning oraz zbadanie jego efektywności na podstawie środowiska **Taxi** z modułu **Gym**. Oprócz tego należało zbadać wpływ hiperparametru szybkości uczenia się na wyniki.\n",
    "\n",
    "### 2. Implementacja\n",
    "\n",
    "Algorytm został zaimplementowany w postaci klasy **Q_learning** w pliku **Qlearn.py**. Podczas inicjalizacji istnieje możliwość wczytania tabeli z wartościami nagród dla poszczególnych stanów (nazywaną dalej **tabelą Q**) z pliku json. Plik musi znajdować się w folderze wykonawczym oraz nosić nazwę **qtable.json**. \n",
    "\n",
    "Główną częścią implementacji jest funkcja *episode*. To w niej znajduje się właściwy algorytm Q-learning. Funkcja prezentuje się następująco:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acede8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode(self, turns, pickMove):\n",
    "    observation_old, info = self.env.reset()\n",
    "    for _ in range(turns):\n",
    "        move = pickMove(observation_old)\n",
    "        observation_new, reward, terminated, truncated, info = self.env.step(move)\n",
    "        self.table[observation_old][move] = self.table[observation_old][move] + self.learning_rate * (reward + self.discount_factor * max(self.table[observation_new]) - self.table[observation_old][move])\n",
    "        observation_old = observation_new\n",
    "        if terminated:\n",
    "            break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bce39e99",
   "metadata": {},
   "source": [
    "Choć możliwym jest wywołanie funkcji *episode* samemu, zalecanym jest używanie funkcji *train*. Po przekazaniu jej ilości epizodów, maksymalnej ilości tur ruchu taksówki oraz wybraniu odpowiedniej strategii, algorytm przeprowadzi określoną ilość ćwiczeń."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c97cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, episodes, turns, strategy):\n",
    "    pickMove = None\n",
    "    if strategy == \"greedy\":\n",
    "        pickMove = self.greedyChoice\n",
    "    elif strategy == \"epsilonGreedy\":\n",
    "        pickMove = self.epsilonGreedyChoice\n",
    "    elif strategy == \"boltzmann\":\n",
    "        pickMove = self.boltzmannChoice\n",
    "    elif strategy == \"count\":\n",
    "        pickMove = self.countChoice\n",
    "    else:\n",
    "        raise ValueError(\"this strategy does not exist\")\n",
    "    for _ in range(episodes):\n",
    "        self.episode(turns, pickMove)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a513125",
   "metadata": {},
   "source": [
    "Poszczególne strategie wyboru ruchu prezentują się następująco:\n",
    "* strategia zachłanna - algorytm wybiera ruch o największej wartości nagrody,\n",
    "* strategia E-zachłanna - algorytm z prawdopodobieństwem E wybiera losowy ruch, a z prawdopodobieństwem 1 - E: ruch zachłanny\n",
    "* strategia oparta na rozkładzie Boltzmanna - każdy ruch otrzymuje określone prawdopodobieństwo na podstawie wartości nagród. Ruch jest następnie wybierany na podstawie tych prawdopodobieństw.\n",
    "* strategia licznikowa - w 20% przypadków wybierany jest ruch, który do tej pory był wybierany najrzadziej. W przeciwnym wypadku używana jest strategia zachłanna."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce098946",
   "metadata": {},
   "source": [
    "W klasie Q_learning znajdują się również dwie metody, używane do testowania działania algorytmu. Są to odpowiednio *test* oraz *episodeTest*. Ta druga zachowuje się tak samo jak *episode*, z tą różnicą iż nie modyfikuje tabeli Q, pozwala więc dokładnie sprawdzić efektywność algorytmu. Zwraca także wynik epizodu - prawdę, jeśli taksówce udało się zawieźć pasażera do celu lub fałsz, jeżeli skończył się na to przeznaczony czas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e269b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def episodeTest(self, turns, pickMove):\n",
    "    \"\"\"\n",
    "    Episode without changing the Qtable. Returns True if the goal has been reached\n",
    "    and False if it hasn't been reached. It also returns the number of turns which it took\n",
    "    to achieve the goal.\n",
    "    \"\"\"\n",
    "    observation_old, info = self.env.reset()\n",
    "    for i in range(turns):\n",
    "        move = pickMove(observation_old)\n",
    "        observation_new, reward, terminated, truncated, info = self.env.step(move)\n",
    "        observation_old = observation_new\n",
    "        if terminated:\n",
    "            return (True, i+1)\n",
    "    return (False, turns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a5ca19d",
   "metadata": {},
   "source": [
    "Metoda *test* jest nieco bardziej skomplikowana. Pozwala ona na przetestowanie każdej z dostępnych strategii. Dla każdej strategii generowane są wyniki, gdzie przedstawione jest ile epizodów zakończyło się porażką, ile sukcesem, oraz średnia ilość tur potrzebna do osiągnięcia tegoż sukcesu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2628ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "def test(self, episodes, turns, useGreedy=False, useEpsilonGreedy=False, useBoltzmann=False, useCount=False):\n",
    "    \"\"\"\n",
    "    Runs the algorithm without modyfing the Qtable. Depending on the function parameters,\n",
    "    it is possible to use every strategy for testing the environment.\n",
    "    Results may vary.\n",
    "    \"\"\"\n",
    "    results = dict()\n",
    "    Strategy = namedtuple('Strategy', ['use', 'f'])\n",
    "    strategies = {\n",
    "        \"greedy\" : Strategy(useGreedy, self.greedyChoice),\n",
    "        \"epsilonGreedy\" : Strategy(useEpsilonGreedy, self.epsilonGreedyChoice),\n",
    "        \"boltzmann\" : Strategy(useBoltzmann, self.boltzmannChoice),\n",
    "        \"count\" : Strategy(useCount, self.countChoice)\n",
    "    }\n",
    "    for name, strategy in strategies.items():\n",
    "        if strategy.use is True:\n",
    "            miniResults = {\"positive\":0, \"negative\":0, \"average\":0}\n",
    "            for _ in range(episodes):\n",
    "                wasAchieved, numOfTurns = self.episodeTest(turns, strategy.f)\n",
    "                if wasAchieved:\n",
    "                    miniResults[\"positive\"] += 1\n",
    "                    miniResults[\"average\"] += numOfTurns\n",
    "                else:\n",
    "                    miniResults[\"negative\"] += 1\n",
    "            if miniResults[\"positive\"] != 0:\n",
    "                miniResults[\"average\"] = miniResults[\"average\"] / miniResults[\"positive\"]\n",
    "            else:\n",
    "                miniResults[\"average\"] = 0\n",
    "            results[f\"{name}\"] = miniResults\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67d71876",
   "metadata": {},
   "source": [
    "### 3. Testy\n",
    "\n",
    "Aby zbadać działanie algorytmu oraz wpływ szybkości uczenia się na jego działanie, stworzyłem cztery funkcje testujące zawarte w pliku **Qtest.py**. Każda z nich testuje jedną z czterech strategii, a następnie generuje odpowiednie wykresy. Gdy strategia sama w sobie posiada pewien parametr który można modyfikować, dla każdego z tych wewnętrznych parametrów tworzony jest nowy wykres.\n",
    "\n",
    "Wykresy pokazują, jak w czasie zmienia się efektywność algorytmu. Test za każdym razem składał się ze 100 epizodów, więc naturalnie efektywność była mierzona tym, ile z tych epizodów zakończyło się sukcesem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14f9f9b7",
   "metadata": {},
   "source": [
    "## Wykresy\n",
    "\n",
    "![greedy strategy](graphs/greedy1.png)\n",
    "\n",
    "![epsilon greedy strategy 0.3](graphs/epsilon_greedy%2Cepsilon%3D0%2C3.png)\n",
    "\n",
    "![epsilon greedy strategy 0.5](graphs/epsilon_greedy%2Cepsilon%3D0%2C5.png)\n",
    "\n",
    "![epsilon greedy strategy 0.7](graphs/epsilon_greedy%2Cepsilon%3D0%2C7.png)\n",
    "\n",
    "![boltzmann strategy 0.1](graphs/boltzmann%2Ctau%3D0%2C1.png)\n",
    "\n",
    "![boltzmann strategy 1](graphs/boltzmann%2Ctau%3D1.png)\n",
    "\n",
    "![boltzmann strategy 10](graphs/boltzmann%2Ctau%3D10.png)\n",
    "\n",
    "![counting strategy](graphs/count.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f7ce59c",
   "metadata": {},
   "source": [
    "# 4. Wnioski\n",
    "\n",
    "Choć posczególne strategie różnią się efektywnością, to istnieje jeden wspólny mianownik. Gdy parametr uczenia się jest równy 1, algorytm uczy się najszybciej. Jest to zgodne z literaturą, gdzie taka wartość określana jest jako optymalna. Im wartość niższa od 1, tym algorytm wolniej się uczy. Co jednak ciekawe, zwiększenie tego parametru do 5 powoduje całkowite załamanie uczenia się. Najprawdopodobniej wynika to z tego, iż przy tak wysokim parametrze uczenia się stara wiedza jest całkowicie zamazywana przez nową. Nowe wartości powinny jedynie delikatnie zmieniać wartość nagrody, a nie stanowić jej większość.\n",
    "\n",
    "Strategia zachłanna i E-zachłanna dają dobre wyniki, choć przy E-zachłannej należy uważać z parameterem E. Gdy jest równy 0.3, daje to najlepsze wyniki. Jest to dobry balans pomiędzy eksploracją przestrzeni a eksploatacją zdobytej już wiedzy. Strategia licznikowa także wykonuje swoje zadanie, choć zauważalnie gorzej niż strategie zachłanne.\n",
    "\n",
    "Najciekawiej jednak działa strategia oparta na rozkładzie Boltzmanna. Niezależnie od parametru tau oraz od parametru uczenia się, wyniki są niezwykle chaotyczne. Nie jest to jednak zachowanie całkowicie losowe. Gdyby tak było, wykres powinien być linią prostą. Algorytm wykonuje czasami poprawne ruchy i czasami osiąga sukces, ale losowość jest najwyraźniej zbyt duża, aby był w stanie osiągać ten cel konsekwentnie.\n",
    "\n",
    "Symptomy nadmiernej losowości można dostrzec na wykresie ze strategią E-zachłanną, z E równym 0.7. Choć algorytm uczy się, dochodząc do efektwności na poziomie 60%, to nie jest w stanie przekroczyć tej bariery. Eksploracja znacznie przeważa nad eksploatacją, więc algorytm nie jest w stanie dopracować tych dobrych ścieżek. Moim zdaniem strategia Boltzmannowska posiada ten sam problem, z tym że w dużo gorszym stopniu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
